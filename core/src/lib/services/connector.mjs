import { readYamlFile, readDirectory, writeFile, writeYamlFile, chown, mkdir, rm } from '#shared/fs'
import { extname, basename } from 'node:path'
// Store
import { store } from '../store.mjs'

export const wanted = () => false

/**
 * Service object holds the various lifecycle hook methods
 */
export const service = {
  name: 'connector',
  hooks: {
    wanted: async () => {
      if (store.config?.connector?.pipelines) {
        if (
          Object.values(store.config.connector.pipelines).filter((pipe) => !pipe.disabled).length >
          0
        )
          return true
      }

      return false
    },
    recreateContainer: () => false,
    restartContainer: () => false,
    /*
     * Before creating the container, write out the logstash.yml file
     * This will be volume-mapped, so we need to write it to
     * disk first so it's available
     */
    preCreate: async () => {
      /*
       * See if logstash.yml on the host OS is present
       */
      const file = '/etc/morio/connector/config/logstash.yml'
      const config = await readYamlFile(file)
      if (config && false === 'fixme') {
        store.log.debug('Connector: Config file exists, no action needed')
      } else {
        store.log.debug('Connector: Creating config file')
        await writeYamlFile(file, store.config.services.connector.logstash, store.log, 0o644)
      }

      /*
       * Make sure the data directory exists, and is writable
       */
      const uid = store.getPreset('MORIO_CONNECTOR_UID')
      await mkdir('/morio/data/connector')
      await chown('/morio/data/connector', uid, uid)

      /*
       * Make sure the pipelines directory exists, and is writable
       */
      await mkdir('/etc/morio/connector/pipelines')
      await chown('/etc/morio/connector/pipelines', uid, uid)

      return true
    },
    preStart: async () => {
      /*
       * Need to write out pipelines, but also remove any that
       * may no longer be there, so we first need to load all
       * pipelines that are on disk
       */
      const currentPipelines = await loadPipelinesFromDisk()
      const wantedPipelines = (Object.keys(store.settings.connector?.pipelines) || []).filter(
        (id) => {
          if (!store.settings?.connector?.pipelines?.[id]) return false
          if (store.settings?.connector?.pipelines?.[id].disabled) return false
          return true
        }
      )

      await createWantedPipelines(wantedPipelines)
      await removeUnwantedPipelines(currentPipelines, wantedPipelines)

      return true
    },
  },
}

const loadPipelinesFromDisk = async () =>
  ((await readDirectory(`/etc/morio/connector/pipelines`)) || [])
    .filter((file) => extname(file) === '.config')
    .map((file) => basename(file).slice(0, -7))
    .sort()

const pipelineFilename = (id) => `${id}.config`

const createWantedPipelines = async (wantedPipelines) => {
  const pipelines = []
  for (const id of wantedPipelines) {
    const config = generatePipelineConfiguration(store.settings.connector.pipelines[id], id)
    if (config) {
      const file = pipelineFilename(id)
      await writeFile(`/etc/morio/connector/pipelines/${file}`, config, store.log)
      store.log.debug(`Created connector pipeline ${id}`)
      pipelines.push({
        'pipeline.id': id,
        'path.config': `/usr/share/logstash/pipeline/${file}`,
      })
    }
  }
  await writeYamlFile(`/etc/morio/connector/config/pipelines.yml`, pipelines, store.log)
}

const removeUnwantedPipelines = async (currentPipelines, wantedPipelines) => {
  for (const id of currentPipelines) {
    if (!wantedPipelines.includes(id)) {
      store.log.debug(`Removing pipeline: ${id}`)
      await rm(`/etc/morio/connector/pipelines/${id}.config`)
    }
  }
}

const generatePipelineConfiguration = (pipeline, pipelineId) => {
  const input = store.settings.connector?.inputs?.[pipeline.input.id] || false
  const output = store.settings.connector?.outputs?.[pipeline.output.id] || false

  if (!input || !output) return false

  return `# This pipeline configuration is auto-generated by Morio core
# Any changes you make to this file will be overwritten
${generateXputConfig(input, pipeline, pipelineId, 'input')}
${generateXputConfig(output, pipeline, pipelineId, 'output')}
`
}

const logstashPluginName = (plugin) =>
  ['morio_local', 'morio_remote'].includes(plugin) ? 'kafka' : plugin

const generateXputConfig = (xput, pipeline, pipelineId, type) =>
  logstash[type]?.[xput.plugin]
    ? logstash[type][xput.plugin](xput, pipeline, pipelineId)
    : `
# ${type === 'input' ? 'Input' : 'Output'}, aka where to ${type === 'input' ? 'read data from' : 'write data to'}
${type} {
  ${logstashPluginName(xput.plugin)} { ${generatePipelinePluginConfig(xput.plugin, xput, pipeline, pipelineId, type)}  }
}
`
const generatePipelinePluginConfig = (plugin, xput, pipeline, pipelineId, type) => {
  let config = ''
  for (const [key, val] of Object.entries(xput)) {
    if (!['id', 'type', 'plugin', 'about'].includes(key)) {
      config += `\n    ${key} => ${JSON.stringify(val)}`
    }
    if (key === 'id') config += `\n    ${key} => ${JSON.stringify(pipelineId + '_' + val)}`
  }

  if (pipeline && type === 'output') {
    if (plugin === 'morio_local') {
      config += `\n    topic => ${JSON.stringify(pipeline.output.topic)}`
    }
  }

  return config + '\n'
}

const logstash = {
  input: {},
  output: {
    morio_local: (xput, pipeline, pipelineId) => `
# Output data to the local Morio deployment
output {
  kafka {
    codec => json
    topic_id => "${pipeline.output.topic}"
    bootstrap_servers => "${store.settings.deployment.nodes.map((node, i) => `broker_${Number(i) + 1}:9092`).join(',')}"
    client_id => "morio_connector"
    id => "${pipelineId}_${xput.id}"
  }
}
`,
  },
}
